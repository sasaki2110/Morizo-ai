"""
Morizo AI - Smart Pantry AI Agent
Copyright (c) 2024 Morizo AI Project. All rights reserved.

This software is proprietary and confidential. Unauthorized copying, modification,
distribution, or use is strictly prohibited without explicit written permission.

For licensing inquiries, contact: [contact@morizo-ai.com]
"""

from fastapi import FastAPI, HTTPException, Depends
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import OpenAI
from supabase import create_client, Client
import os
import json
import asyncio
from typing import Optional, Dict, Any
from dotenv import load_dotenv
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

app = FastAPI(title="Morizo AI", description="Smart Pantry AI Agent with MCP Integration")

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æœ¬ç•ªç’°å¢ƒã§ã¯é©åˆ‡ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã«åˆ¶é™
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# OpenAI APIè¨­å®š
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
model_name = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# Supabaseè¨­å®š
supabase_url = os.getenv("SUPABASE_URL")
supabase_key = os.getenv("SUPABASE_KEY")
supabase: Client = create_client(supabase_url, supabase_key) if supabase_url and supabase_key else None

# MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š
class MCPClient:
    """MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.server_params = StdioServerParameters(
            command="python",
            args=["db_mcp_server_stdio.py"]
        )
    
    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """MCPãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã—"""
        try:
            async with stdio_client(self.server_params) as (read, write):
                async with ClientSession(read, write) as session:
                    await session.initialize()
                    result = await session.call_tool(tool_name, arguments=arguments)
                    
                    if result and hasattr(result, 'content') and result.content:
                        return json.loads(result.content[0].text)
                    else:
                        return {"success": False, "error": "No result from MCP tool"}
        except Exception as e:
            return {"success": False, "error": f"MCP tool error: {str(e)}"}

# ã‚°ãƒ­ãƒ¼ãƒãƒ«MCPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
mcp_client = MCPClient()

# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
print(f"ğŸš€ Morizo AI Server starting...")
print(f"ğŸ“‹ Using OpenAI Model: {model_name}")
print(f"ğŸ”‘ OpenAI API Key configured: {'Yes' if os.getenv('OPENAI_API_KEY') else 'No'}")
print(f"ğŸ” Supabase configured: {'Yes' if supabase else 'No'}")
print(f"ğŸ”— MCP Integration: Enabled")

# èªè¨¼è¨­å®š
security = HTTPBearer()

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """
    Supabaseãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ¤œè¨¼ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’è¿”ã™
    """
    if not supabase:
        raise HTTPException(
            status_code=500, 
            detail="Supabase not configured"
        )
    
    try:
        # ãƒˆãƒ¼ã‚¯ãƒ³ã®å‰å‡¦ç†ï¼ˆè§’æ‹¬å¼§ã®é™¤å»ï¼‰
        raw_token = credentials.credentials
        if raw_token.startswith('[') and raw_token.endswith(']'):
            raw_token = raw_token[1:-1]
        
        print(f"ğŸ” [DEBUG] Raw token received: {raw_token[:50]}...")
        print(f"ğŸ” [DEBUG] Token length: {len(raw_token)}")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
        response = supabase.auth.get_user(raw_token)
        
        if response.user is None:
            print(f"âŒ [ERROR] User is None in response")
            raise HTTPException(
                status_code=401,
                detail="Invalid authentication token"
            )
        
        print(f"âœ… [SUCCESS] User authenticated: {response.user.email}")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¾æ›¸ã§è¿”ã™
        return {
            "user": response.user,
            "raw_token": raw_token
        }
    except Exception as e:
        print(f"âŒ [ERROR] Authentication failed: {str(e)}")
        raise HTTPException(
            status_code=401,
            detail=f"Authentication failed: {str(e)}"
        )

class ChatRequest(BaseModel):
    message: str
    user_id: Optional[str] = None

class ChatResponse(BaseModel):
    model_config = {"protected_namespaces": ()}
    
    response: str
    success: bool
    model_used: str
    user_id: Optional[str] = None

# å‹•çš„MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã™ã¹ã¦ã®åœ¨åº«æ“ä½œã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã€
# å€‹åˆ¥ã®Pydanticãƒ¢ãƒ‡ãƒ«ã¯ä¸è¦ã«ãªã‚Šã¾ã—ãŸã€‚
# ã™ã¹ã¦ã®æ“ä½œã¯ /chat ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆçµŒç”±ã§è‡ªç„¶è¨€èªã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

@app.get("/")
async def root():
    return {"message": "Morizo AI is running!", "mcp_integration": "enabled"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "Morizo AI", "mcp_integration": "enabled"}

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest, auth_data = Depends(verify_token)):
    """
    Morizo AI ReAct Agent - è¦³å¯Ÿâ†’æ€è€ƒâ†’æ±ºå®šâ†’è¡Œå‹•ã®ãƒ«ãƒ¼ãƒ—ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†
    """
    try:
        current_user = auth_data["user"]
        raw_token = auth_data["raw_token"]
        
        print(f"\n=== Morizo AI ReAct Agent é–‹å§‹ ===")
        print(f"ğŸ” [è¦³å¯Ÿ] ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: {request.message}")
        print(f"   User: {current_user.email}")
        print(f"   User ID: {current_user.id}")
        
        if not os.getenv("OPENAI_API_KEY"):
            raise HTTPException(status_code=500, detail="OpenAI API key not configured")
        
        # === æ€è€ƒãƒ•ã‚§ãƒ¼ã‚º ===
        print(f"ğŸ§  [æ€è€ƒ] MCPã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å‹•çš„ã«ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—ä¸­...")
        
        # MCPã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰å‹•çš„ã«ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
        available_tools = []
        try:
            async with stdio_client(mcp_client.server_params) as (read, write):
                async with ClientSession(read, write) as session:
                    await session.initialize()
                    
                    # ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
                    tools_response = await session.list_tools()
                    
                    if tools_response and hasattr(tools_response, 'tools'):
                        for tool in tools_response.tools:
                            available_tools.append(f"- {tool.name}: {tool.description}")
                    
                    print(f"ğŸ§  [æ€è€ƒ] åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«: {len(available_tools)}å€‹")
                    
        except Exception as e:
            print(f"âŒ [ã‚¨ãƒ©ãƒ¼] ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆå–å¾—å¤±æ•—: {str(e)}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ
            available_tools = [
                "- inventory_list: åœ¨åº«ä¸€è¦§ã‚’å–å¾—",
                "- inventory_add: åœ¨åº«ã«ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿½åŠ ",
                "- inventory_get: ç‰¹å®šã®ã‚¢ã‚¤ãƒ†ãƒ ã®è©³ç´°ã‚’å–å¾—",
                "- inventory_update: åœ¨åº«ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ›´æ–°",
                "- inventory_delete: åœ¨åº«ã‚¢ã‚¤ãƒ†ãƒ ã‚’å‰Šé™¤"
            ]
        
        # LLMã«ãƒ„ãƒ¼ãƒ«é¸æŠã‚’ä¾é ¼
        tools_list = "\n".join(available_tools)
        tool_selection_prompt = f"""
ã‚ãªãŸã¯Morizoã¨ã„ã†ã‚¹ãƒãƒ¼ãƒˆãƒ‘ãƒ³ãƒˆãƒªãƒ¼ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã‚’åˆ†æã—ã€é©åˆ‡ãªãƒ„ãƒ¼ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«:
{tools_list}
- llm_chat: ä¸€èˆ¬çš„ãªä¼šè©±ã‚„è³ªå•

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚: "{request.message}"

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
{{
    "tool": "ãƒ„ãƒ¼ãƒ«å",
    "reasoning": "é¸æŠç†ç”±",
    "parameters": {{
        "item_name": "ã‚¢ã‚¤ãƒ†ãƒ åï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰",
        "quantity": æ•°é‡ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰,
        "unit": "å˜ä½ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰",
        "storage_location": "ä¿ç®¡å ´æ‰€ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰"
    }}
}}
"""
        
        try:
            tool_response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": tool_selection_prompt}],
                max_tokens=200,
                temperature=0.1
            )
            
            tool_decision = tool_response.choices[0].message.content
            print(f"ğŸ§  [æ€è€ƒ] LLMåˆ¤æ–­: {tool_decision}")
            
            # JSONè§£æ
            import json
            try:
                tool_data = json.loads(tool_decision)
                selected_tool = tool_data.get("tool", "llm_chat")
                reasoning = tool_data.get("reasoning", "")
                parameters = tool_data.get("parameters", {})
                
                print(f"ğŸ¯ [æ±ºå®š] é¸æŠã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«: {selected_tool}")
                print(f"ğŸ¯ [æ±ºå®š] ç†ç”±: {reasoning}")
                
            except json.JSONDecodeError:
                print(f"âš ï¸ [è­¦å‘Š] JSONè§£æå¤±æ•—ã€LLMãƒãƒ£ãƒƒãƒˆã‚’ä½¿ç”¨")
                selected_tool = "llm_chat"
                parameters = {}
                
        except Exception as e:
            print(f"âŒ [ã‚¨ãƒ©ãƒ¼] LLMå‘¼ã³å‡ºã—å¤±æ•—: {str(e)}")
            selected_tool = "llm_chat"
            parameters = {}
        
        # === è¡Œå‹•ãƒ•ã‚§ãƒ¼ã‚º ===
        print(f"ğŸ” [è¡Œå‹•] {selected_tool}ã‚’å®Ÿè¡Œä¸­...")
        
        if selected_tool == "llm_chat":
            print(f"ğŸ” [è¡Œå‹•] LLMãƒãƒ£ãƒƒãƒˆã‚’å®Ÿè¡Œ")
            ai_response = await get_llm_response(request.message, current_user)
            
        elif selected_tool != "llm_chat":
            print(f"ğŸ” [è¡Œå‹•] MCPã§{selected_tool}ã‚’å®Ÿè¡Œ")
            try:
                # å‹•çš„ã«MCPãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã—
                mcp_arguments = {"token": raw_token}
                
                # LLMãŒæŠ½å‡ºã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                if parameters:
                    mcp_arguments.update(parameters)
                
                print(f"ğŸ” [è¡Œå‹•] å¼•æ•°: {mcp_arguments}")
                
                mcp_result = await mcp_client.call_tool(
                    selected_tool,
                    arguments=mcp_arguments
                )
                
                if mcp_result.get("success"):
                    print(f"âœ… [æˆåŠŸ] {selected_tool}å®Ÿè¡Œå®Œäº†")
                    
                    # å‹•çš„ãªçµæœå‡¦ç†
                    if mcp_result.get("data"):
                        # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯ã€LLMã«çµæœã‚’æ•´å½¢ã—ã¦ã‚‚ã‚‰ã†
                        data_str = json.dumps(mcp_result["data"], ensure_ascii=False, indent=2)
                        formatting_prompt = f"""
ä»¥ä¸‹ã®{selected_tool}ã®å®Ÿè¡Œçµæœã‚’ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§æ•´å½¢ã—ã¦ãã ã•ã„ã€‚

å®Ÿè¡Œçµæœ:
{data_str}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚: "{request.message}"

è‡ªç„¶ã§è¦ªã—ã¿ã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
                        
                        try:
                            format_response = client.chat.completions.create(
                                model="gpt-4o-mini",
                                messages=[{"role": "user", "content": formatting_prompt}],
                                max_tokens=500,
                                temperature=0.3
                            )
                            ai_response = format_response.choices[0].message.content
                        except Exception as e:
                            print(f"âš ï¸ [è­¦å‘Š] çµæœæ•´å½¢å¤±æ•—: {str(e)}")
                            ai_response = mcp_result.get("message", f"{selected_tool}ãŒæ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã¾ã—ãŸã€‚")
                    else:
                        # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãã®ã¾ã¾è¡¨ç¤º
                        ai_response = mcp_result.get("message", f"{selected_tool}ãŒæ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã¾ã—ãŸã€‚")
                else:
                    print(f"âŒ [ã‚¨ãƒ©ãƒ¼] MCPå¤±æ•—: {mcp_result.get('error')}")
                    ai_response = f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚{selected_tool}ã®å®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {mcp_result.get('error')}"
                    
            except Exception as e:
                print(f"âŒ [ã‚¨ãƒ©ãƒ¼] MCPå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
                ai_response = f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚{selected_tool}ã®å®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        
        else:
            print(f"ğŸ” [è¡Œå‹•] LLMãƒãƒ£ãƒƒãƒˆã‚’å®Ÿè¡Œ")
            ai_response = await get_llm_response(request.message, current_user)
        
        print(f"âœ… [å®Œäº†] æœ€çµ‚å¿œç­”: {ai_response}")
        print(f"=== Morizo AI ReAct Agent çµ‚äº† ===\n")
        
        return ChatResponse(
            response=ai_response,
            success=True,
            model_used="gpt-4o-mini",
            user_id=current_user.id
        )
        
    except Exception as e:
        print(f"âŒ [ERROR] Chat processing error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"AI processing error: {str(e)}")

async def get_llm_response(message: str, current_user) -> str:
    """LLMã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—"""
    try:
        model_name = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯Morizoã¨ã„ã†åå‰ã®ã‚¹ãƒãƒ¼ãƒˆãƒ‘ãƒ³ãƒˆãƒªãƒ¼ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚é£Ÿæç®¡ç†ã¨ãƒ¬ã‚·ãƒ”ææ¡ˆã‚’æ‰‹ä¼ã„ã¾ã™ã€‚æ—¥æœ¬èªã§è¦ªã—ã¿ã‚„ã™ãå›ç­”ã—ã¦ãã ã•ã„ã€‚"},
                {"role": "user", "content": message}
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        print(f"âŒ [ERROR] LLM response error: {str(e)}")
        return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚AIå¿œç­”ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

# å‹•çš„MCPã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã™ã¹ã¦ã®åœ¨åº«æ“ä½œã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã€
# å€‹åˆ¥ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯ä¸è¦ã«ãªã‚Šã¾ã—ãŸã€‚
# ã™ã¹ã¦ã®æ“ä½œã¯ /chat ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆçµŒç”±ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)